---
title: 决策树构造原理
date: 2020-07-05 12:34:03
categories: 算法
tags: 决策树

---

### Hexo插入图片测试

在Hexo中插入图片，可以通过将 `config.yml` 文件中的 `post_asset_folder` 选项设为 `true` 来打开。

### 决策树模型

决策树是一种非常受欢迎的分类和预测工具，本文主要讨论用于分类的决策树，如结果是Yes or No，或者属于ABC哪种类别等*非连续，离散型*预测。决策树是一个树状结构的流程图，由下面几个元素构成，通过一幅图说明：

![tree](tree.png)

非叶子节点（内部节点）：表示在一个特征或属性，如上图中Outlook，Humidity，wind

分支：表示特征或属性在值域上的一个结果，如天气的Sunny，Overcast，Rain

叶子节点（终端节点）：决策的结果，拥有一个分类标签，如上图中YES/NO

### 决策树生成

构造的基本思想是随着树深度的增加，内部节点的熵迅速降低（分裂后子集就越"纯"），越快越好，这样能使树的高度最矮。

下面通过一个例子构造出决策树，重点是记录实现过程

![PlayTennis](E:\hexoblog2020\zkkio-hexo-blog\source\_posts\决策树构造原理\PlayTennis.png)

假设在上面的数据集E中，有4中类别的数据用ABCD顺序表示，下面说明如何使用ID3算法选择数据集中哪个特征作为第一个节点，首先说明用到的几个概念：

#### 信息增益

信息熵表示的是不确定度。均匀分布时，不确定度最大，此时熵就最大。当选择某个特征对数据集进行分类时，分类后的数据集信息熵会比分类前的小，其差值表示为`信息增益`。信息增益可以衡量<u>某个特征对分类结果的影响</u>大小。

![img04](E:\hexoblog2020\zkkio-hexo-blog\source\_posts\决策树构造原理\img04.png)

#### 特征作用前的熵 （经验熵）

![img02](E:\hexoblog2020\zkkio-hexo-blog\source\_posts\决策树构造原理\img02.png)

#### 特征作用后的熵

![img03](E:\hexoblog2020\zkkio-hexo-blog\source\_posts\决策树构造原理\img03.png)

#### ID3计算过程

上面的数据集在没有任何特征作用下，只通过观察是否去打球的概率计算的`熵`(经验熵)为：

H(E)  = -(9/14)log_2(9/14)-(5/14)log_2(5/14) = 0.940

##### 确定哪一个特征做根节点

我们需要分别计算在已知的4种特征情况下，是否打球的熵值都为多少，这样能区分每个特征对结果的重要性。首先计算如果**outlook**在不同取值时打球和不打球的熵值：

outlook = sunny --> 共5次，2次打球，3次不打球。熵 ：-(2/5)log_2(2/5)-(3/5)log_2(3/5) = 0.971 

outlook = overcost --> 共4次，都去打球了， 熵：-(1)log_2(1) = 0

outlook = rainy --> 共5次，3次打球，2次没去。熵：-(3/5)log_2(3/5)-(2/5)log_2(2/5) = 0.971

🏐所以当已知outlook的情况下，打球的不确定性，即特征作用后的熵为：5/14 * 0.971 + 4/14 * 0 + 5/14 * 0.971 = 0.693

根据公式计算出 信息增益 （gain）为 ： g(E, A) = 0.940 - 0.693 = 0.247。

同理可得已知其他特征时计算的熵值：g(E, B) = 0.029	g(E, C) = 0.152	g(E, D) = 0.048

结论：g(E, A) 最大，说明使用特征A能使系统的熵下降最快，所以第一个节点（根节点）就选择`outlook`

##### 接下来计算第二个节点

在BCD三个特征下，计算系统的熵， 和每个特征作为已知条件下的熵，最终得到信息增益，选择最大的作为第二个节点。

以此类推到系统熵下`降为0`结束构造。

上面这种通过信息增益选择特征构造决策树的方式就是ID3算法。

### 扩展（待补充）

C4.5 -- 信息增益率

CART -- 

### 剪枝（待补充）

